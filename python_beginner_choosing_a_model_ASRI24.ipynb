{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6967fdb4",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jcausey-astate/ASRI-2024/blob/main/python_beginner_choosing_a_model_ASRI24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ae92c",
   "metadata": {
    "id": "cb7ae92c"
   },
   "source": [
    "# Choosing and Evaluating Machine Learning Models in Python (Beginner)\n",
    "\n",
    "## ASRI 2024\n",
    "\n",
    "![](https://jcausey-astate.github.io/ASRI-2024/images/choosing_and_evaluating_ml_models_in_python_title_card.svg)\n",
    "\n",
    "This notebook shows some introductory examples from the \"Choosing and Evaluating\n",
    "Machine Learning Models in Python\" workshop session.\n",
    "\n",
    "The notebook uses the following modules:\n",
    "\n",
    "* `matplotlib`  : Provides basic graphing/charting.\n",
    "* `numpy`       : Allows matrix and vector/array math.\n",
    "* `pandas`      : Provides DataFrame functionality.\n",
    "* `seaborn`     : Works with `matplotlib` to provide nicer graphs.\n",
    "* `sklearn`     : Scikit-Learn provides machine learning and data manipulation\n",
    "  tools.\n",
    "\n",
    "We will rely heavily on the Scikit-Learn library for models, metrics, and\n",
    "experimental design tools.  See the full documentation for this fantastic\n",
    "library at <https://scikit-learn.org>.\n",
    "\n",
    "---\n",
    "\n",
    "In this workshop, we will focus on the big picture questions of model selection and\n",
    "evaluation:\n",
    "\n",
    "* **Model Selection :** How to you begin to choose a model?\n",
    "  * Prediction vs Exploration\n",
    "  * Regression vs Classification\n",
    "  * Supervised vs Unsupervised\n",
    "  * General model types and assumptions\n",
    "  * Some common and useful models to consider.\n",
    "* **Evaluation Metrics :** How do you know when your model is doing a good job?\n",
    "\n",
    "## Model Selection : How to you begin to choose a model?\n",
    "\n",
    "### Prediction vs Exploration\n",
    "\n",
    "**Prediction** means that you have some value, label, or other concept that you want to be able to predict in the future, given some measurements or information (we will call these \"features\" or \"random variables\" or \"predictors\") from a sample.  For example, you might want to predict whether a mushroom is edible based on observed facts about it such as gill type, spore print color, cap color, etc.\n",
    "\n",
    "A predictive analysis implies that you _already know what you are looking for_ -- you have a value that is important to you (but might be hard to directly measure), and some predictive measurements that you believe might be able to determine that value.\n",
    "\n",
    "**Exploration** means that you are more interested in using a dataset to discover interesting facts that might suggest something about how the world works.  For example, you might want to figure out whether you can identify separate groups of plants within a field that are growing differently in some way from other plants.\n",
    "\n",
    "An exploratory analysis is an exercise in _curiosity_.  You are looking for interesting relationships or facts in a dataset that might be useful to you in some way.\n",
    "\n",
    "### Regression vs Classification\n",
    "\n",
    "**_Regression_** models try to compute a _continuous_ output value (dependent variable) for a given _sample_.\n",
    "\n",
    "* **_Continuous_** values are numeric values that are allowed to take on any value within some range.\n",
    "* A **_sample_** consists of all of the experimental information gathered for one\n",
    "item in the dataset.  Sometimes a _sample_ is called an _object_ or _item_.\n",
    "Usually samples are arranged as _rows_ in tabular datasets (CSV files, Excel\n",
    "spreadsheets, or similar).\n",
    "\n",
    "**_Classification_** is the process of determining a _categorical label_ given\n",
    "the _random variables_ for a given _sample_.\n",
    "\n",
    "* **_Categorical_** values are allowed to take on only a finite (usually small)\n",
    "set of values.  Categorical variables are usually non-numeric, but are sometimes\n",
    "encoded as numbers.  Sometimes we refer to values of this type as _labels_,\n",
    "_factors_, or _classes_.\n",
    "\n",
    "A **_random variable_**, sometimes called an _input variable_, _measurement_, or\n",
    "_feature_, is the recorded value for some property of the sample that was\n",
    "measured in the experiment, e.g. \"height\", \"age\", \"flower color\", etc.\n",
    "\n",
    "### Supervised vs Unsupervised\n",
    "\n",
    "**_Supervised_** models require a training dataset that has the _correct_ output value (dependent variable) already determined.  The model is trained (or \"fit\") so that the correct output value is produced given the random variables in the sample for each sample in the training data.\n",
    "\n",
    "* In Scikit-Learn: <https://scikit-learn.org/stable/supervised_learning.html>\n",
    "\n",
    "**_Unsupervised_** models are able to produce an estimate for the dependent variable without the need to provide a pre-labeled training dataset.  Some unsupervised models focus on establishing similarities between groups of samples, rather than computing an output value.  (Clustering models do this, for example.)\n",
    "\n",
    "* In Scikit-Learn: <https://scikit-learn.org/stable/unsupervised_learning.html>\n",
    "\n",
    "### General model types and assumptions\n",
    "\n",
    "* **Parametric models**\n",
    "  * Parametric models are statistical models that make assumptions about the underlying distribution of the data being modeled.  In parametric models, the relationship between the input variables and the output variable is represented by a fixed equation with a predefined number of parameters.\n",
    "* **Nonparametric models**\n",
    "  * Nonparametric models are a type of statistical model that does not make any assumptions about the underlying distribution of the data being modeled. These models do not have a fixed number of parameters, and the complexity of the model increases as the amount of data increases. Nonparametric models are particularly useful when the underlying distribution of the data is unknown or when the data is very complex and nonlinear.\n",
    "\n",
    "---\n",
    "\n",
    "* **Linear models**\n",
    "  * As the name implies, linear models imply a linear relationship between input features and the dependent variable:  $y=mx+b$.\n",
    "  * Linear models can be extended to allow the dependent variable to follow a non-normal distribution through the use of a _link function_.  Models of this type are called _generalized linear models_ (GLMs); the Logistic Regression model is a common example.\n",
    "* **Kernel-based models**\n",
    "  * Kernel-based models transform the input variables into a potentially non-linear high-dimensional space before applying multi-linear modeling methods to solve for the dependent variable.  This allows the model to work on problems where simple linear models would not perform well.\n",
    "  * A common example is the Support Vector Machine (SVM).\n",
    "* **Tree-based models**\n",
    "  * Tree-based models are based around the concept of a _decision tree_, which is a structure in which random variables are tested (a decision is made) and the next action to take depends on the result of that decision.  Drawing such a model on paper results in a tree-like structure where each _interior node_ is a decision and each _leaf node_ is a label or regression result.\n",
    "* **Neural Network models**\n",
    "  * Neural-network models rely on layers of artificial \"neurons\", each of which is a linear model followed by a nonlinear _activation function_.  Neural networks tend to contain large numbers of trainable parameters compared to other parametric models, and are capable of fitting very complex datasets with very complex outputs, but require intensive training to reach good performance.\n",
    "  * Neural networks are often characterized by the number of layers (more than one hidden layer ➡️ \"deep learning\") and the way layers are connected (\"fully connected\", \"convolutional\", \"recurrent\", \"transformer\", etc.).\n",
    "* **Clustering models**\n",
    "  * Clustering models are used primarily for _exploratory analysis_.  They do not require a known labeling for the data, and work by establishing groupings within the samples based on some similarity measure.\n",
    "  * Common examples are k-Means, Hierarchical Clustering, DBSCAN, Mean Shift.\n",
    "* **Ensemble models**\n",
    "  * Ensemble models models that use _collections_ of simpler models internally, then reach a final output label or regression result by _consensus_.  By utilizing many simple models, ensembles are often able to both reach higher peak performance and also lower variance than single models.\n",
    "  * Common examples are Random Forests, Boosting models (AdaBoost), and Gradient Boosted Tree models (XGBoost, LightGBM).\n",
    "\n",
    "## Evaluation Metrics :  How do you know when your model is doing a good job?\n",
    "\n",
    "#### Let's see some code! First, we will import the python packages we need for this workshop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf16ca",
   "metadata": {
    "id": "a6cf16ca"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.io import arff\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281dd7f9",
   "metadata": {
    "id": "281dd7f9"
   },
   "source": [
    "#### Now, we need to set up some datasets.  You can run these cells and essentially ignore the code there for now -- come back to it later after you've looked at the discussion of the different metrics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256fc5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f256fc5e",
    "outputId": "99b6768b-5ca0-4875-dd3f-8670283f66b2"
   },
   "outputs": [],
   "source": [
    "# The classification dataset is \"Dry Bean\" from https://archive.ics.uci.edu/dataset/602/dry+bean+dataset\n",
    "# Download the dataset (but only if it hasn't already been done).\n",
    "! [[ -f dry+bean+dataset.zip ]] || wget https://archive.ics.uci.edu/static/public/602/dry+bean+dataset.zip\n",
    "! [[ -d DryBeanDataset ]] || unzip dry+bean+dataset.zip\n",
    "\n",
    "# Load the data.\n",
    "data = arff.loadarff(\"DryBeanDataset/Dry_Bean_Dataset.arff\")\n",
    "beans = pd.DataFrame(data[0])\n",
    "beans[\"Class\"] = beans[\"Class\"].str.decode(\"utf-8\")\n",
    "\n",
    "# We will keep only three classes: \"SIRA, CALI, and BOMBAY\".\n",
    "beans = beans[beans[\"Class\"].isin([\"SIRA\", \"CALI\", \"BOMBAY\"])]\n",
    "# Put the label column name and predictor column names in variables:\n",
    "beans_label_col = \"Class\"\n",
    "# Choose only two variables for this tutorial (and not the best ones, by far).\n",
    "beans_var_cols = [\"Extent\", \"Eccentricity\"]\n",
    "# We will also create a binary version (one with only two classes).\n",
    "bin_beans = beans.copy()\n",
    "bin_beans.loc[bin_beans[\"Class\"] != \"SIRA\", \"Class\"] = \"NOT_SIRA\"\n",
    "\n",
    "# Make a simple 80/20 train/test split:\n",
    "beans_train, beans_test = train_test_split(\n",
    "    beans, test_size=0.20, stratify=beans[\"Class\"], random_state=1\n",
    ")\n",
    "bin_beans_train, bin_beans_test = train_test_split(\n",
    "    bin_beans, test_size=0.20, stratify=beans[\"Class\"], random_state=1\n",
    ")\n",
    "\n",
    "beans_gt = beans_test[beans_label_col].values\n",
    "bin_beans_gt = bin_beans_test[beans_label_col].values\n",
    "\n",
    "# Print counts of each bean type:\n",
    "print(\n",
    "    f\"Counts by class in train set (multiclass):\\n{beans_train['Class'].value_counts()}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\n\\nCounts by class in test set (multiclass):\\n{beans_test['Class'].value_counts()}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\n\\nCounts by class in train set (binary):\\n{bin_beans_train['Class'].value_counts()}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\n\\nCounts by class in test set (binary):\\n{bin_beans_test['Class'].value_counts()}\"\n",
    ")\n",
    "# Create a simple linear (logistic) regression classifier...\n",
    "lm = LogisticRegression(max_iter=1000)\n",
    "# ... And train it on the multiclass problem...\n",
    "lm.fit(beans_train[beans_var_cols], beans_train[beans_label_col])\n",
    "beans_preds = lm.predict(beans_test[beans_var_cols])\n",
    "beans_probas = lm.predict_proba(beans_test[beans_var_cols])\n",
    "mc_labels = lm.classes_\n",
    "# ... And also on the binary classification problem.\n",
    "lm.fit(bin_beans_train[beans_var_cols], bin_beans_train[beans_label_col])\n",
    "bin_beans_preds = lm.predict(bin_beans_test[beans_var_cols])\n",
    "bin_beans_probas = lm.predict_proba(bin_beans_test[beans_var_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d93ef",
   "metadata": {
    "id": "640d93ef"
   },
   "outputs": [],
   "source": [
    "# The regression dataset is \"MPG\" from https://archive.ics.uci.edu/dataset/9/auto+mpg\n",
    "# Download the dataset (but only if it hasn't already been done).\n",
    "! [[ -f auto+mpg.zip ]] || wget https://archive.ics.uci.edu/static/public/9/auto+mpg.zip\n",
    "! [[ -f auto-mpg.data ]] || unzip auto+mpg.zip\n",
    "\n",
    "# Load the data.\n",
    "mpg = pd.read_csv(\n",
    "    \"auto-mpg.data\",\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    usecols=[0, 2, 3, 4],\n",
    "    names=[\"mpg\", \"disp\", \"hp\", \"weight\"],\n",
    "    na_values=[\"?\"],\n",
    ")\n",
    "mpg = mpg.dropna(axis=0)\n",
    "mpg_label_col = \"mpg\"\n",
    "mpg_var_cols = [\"disp\", \"hp\", \"weight\"]\n",
    "# We should really standardize the predictors, but for now, we'll ignore that.\n",
    "\n",
    "# Make a simple 80/20 train/test split:\n",
    "mpg_train, mpg_test = train_test_split(mpg, test_size=0.20, random_state=1)\n",
    "mpg_gt = mpg_test[mpg_label_col].values\n",
    "\n",
    "rm = LinearRegression()\n",
    "rm.fit(mpg_train[mpg_var_cols], mpg_train[mpg_label_col])\n",
    "mpg_preds = rm.predict(mpg_test[mpg_var_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe4a91",
   "metadata": {
    "id": "76fe4a91"
   },
   "source": [
    "### Classification Metrics\n",
    "\n",
    "A very good source of information about metrics for binary classifiers is available on Wikipedia:\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers>\n",
    "\n",
    "#### Some definitions\n",
    "\n",
    "* **Condition Positive (P)**\n",
    "  * The number of real positive cases in the data\n",
    "* **Condition Negative (N)**\n",
    "  * The number of real negative cases in the data\n",
    "* **True Positive (TP)**\n",
    "  * A test result that correctly indicates the presence of a condition or characteristic\n",
    "* **True Negative (TN)**\n",
    "  * A test result that correctly indicates the absence of a condition or characteristic\n",
    "* **False Positive (FP)**\n",
    "  * A test result which wrongly indicates that a particular condition or attribute is present\n",
    "* **False Negative (FN)**\n",
    "  * A test result which wrongly indicates that a particular condition or attribute is absent\n",
    "\n",
    "#### Metrics with examples\n",
    "\n",
    "* **Recall    (REC)**\n",
    "  * $\\frac{\\mathrm{TP}}{\\mathrm{P}}$  or  $\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}$\n",
    "  * In Scikit-Learn:  `sklearn.metrics.recall_score()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9457b0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9457b0c",
    "outputId": "fd65c028-d733-42da-a5a5-2be595f040de"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"REC (binary): {recall_score(bin_beans_gt, bin_beans_preds, pos_label='SIRA'):0.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"REC (multiclass): {recall_score(beans_gt, beans_preds, average='weighted'):0.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb9111",
   "metadata": {
    "id": "a0bb9111"
   },
   "source": [
    "* **Precision (PREC)**\n",
    "  * $\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c6c791",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49c6c791",
    "outputId": "a6c0a6db-ee09-4ff1-e18a-34f39556c8fc"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"PREC (binary): {precision_score(bin_beans_gt, bin_beans_preds, pos_label='SIRA'):0.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"PREC (multiclass): {precision_score(beans_gt, beans_preds, average='weighted'):0.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98090cc",
   "metadata": {
    "id": "d98090cc"
   },
   "source": [
    "* **Accuracy  (ACC)**\n",
    "  * $\\frac{\\mathrm{TP}+\\mathrm{TN}}{\\mathrm{P}+\\mathrm{N}}$ or $\\frac{\\mathrm{TP}+\\mathrm{TN}}{\\mathrm{TP}+\\mathrm{TN}+\\mathrm{FP}+\\mathrm{FN}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067206a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e067206a",
    "outputId": "51b05ff8-234b-4fab-e5d0-aaac21cbd8ba"
   },
   "outputs": [],
   "source": [
    "print(f\"ACC (binary): {accuracy_score(bin_beans_gt, bin_beans_preds):0.3f}\")\n",
    "print(f\"ACC (multiclass): {accuracy_score(beans_gt, beans_preds):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33cbf08",
   "metadata": {
    "id": "b33cbf08"
   },
   "source": [
    "* **Balanced Accuracy (BA)**\n",
    "  * $\\frac{1}{2}(\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}} + \\frac{\\mathrm{TN}}{\\mathrm{TN}+\\mathrm{FP}})$\n",
    "  * For multiclass, see <https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711556d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "711556d1",
    "outputId": "e139d538-add0-48bf-a96f-7b6aab9a73f8"
   },
   "outputs": [],
   "source": [
    "print(f\"BA (binary): {balanced_accuracy_score(bin_beans_gt, bin_beans_preds):0.3f}\")\n",
    "print(f\"BA (multiclass): {balanced_accuracy_score(beans_gt, beans_preds):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb26785",
   "metadata": {
    "id": "aeb26785"
   },
   "source": [
    "* **F1 score  (F1)** is the harmonic mean of precision and recall.\n",
    "  * $\\frac{2 \\mathrm{TP}}{2 \\mathrm{TP}+\\mathrm{FP}+\\mathrm{FN}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa8344",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0faa8344",
    "outputId": "84ff2cb4-6fd8-4e8b-f715-f30f49bf00be"
   },
   "outputs": [],
   "source": [
    "print(f\"F1 (binary): {f1_score(bin_beans_gt, bin_beans_preds, pos_label='SIRA'):0.3f}\")\n",
    "print(f\"F1 (multiclass): {f1_score(beans_gt, beans_preds, average='weighted'):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84fedb7",
   "metadata": {
    "id": "c84fedb7"
   },
   "source": [
    "* **ROC curve** and **Area Under the ROC Curve (AUROC)**\n",
    "  * <https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/><br>![<https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/>](https://glassboxmedicine.files.wordpress.com/2019/02/roc-curve-v2.png)\n",
    "  * Integrating the area under the ROC curve gives you the numeric metric _AUROC_.  It is in the range $[0,1]$, and higher is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca5733e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "2ca5733e",
    "outputId": "ff0b31d7-087d-4efa-b55f-69f67b8626db"
   },
   "outputs": [],
   "source": [
    "# Area under the ROC Curve:\n",
    "auroc_bin = roc_auc_score(\n",
    "    bin_beans_gt, bin_beans_probas[:, 1]\n",
    ")  # 'SIRA' is in column index 1\n",
    "auroc_mc = roc_auc_score(beans_gt, beans_probas, multi_class=\"ovr\")\n",
    "\n",
    "print(f\"AUROC (binary): {auroc_bin:0.3f}\")\n",
    "print(f\"AUROC (multiclass): {auroc_mc:0.3f}\")\n",
    "\n",
    "\n",
    "# A function to plot the ROC curve:\n",
    "def plot_roc_curve(y_true, y_pred_proba, auc, desc=\"\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba, pos_label=\"SIRA\")\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={auc:0.3f}\")\n",
    "    plt.title(f\"ROC {desc}\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_roc_curve(bin_beans_gt, bin_beans_probas[:, 1], auroc_bin, \"(binary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0551e",
   "metadata": {
    "id": "16c0551e"
   },
   "source": [
    "* **Confusion Matrix**\n",
    "  * Visual grid showing the correct labeling (rows) versus the predicted labeling (columns), with the number in each cell representing the number of samples predicted for each (ground_truth / prediction) combination.\n",
    "  * The major diagonal shows _correct_ labelings, all others show _incorrect_ labelings.\n",
    "  * Most often used in multi-class evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f671c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "3b8f671c",
    "outputId": "2315ef86-d974-4ce0-dfca-c17101f4d13a"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for the multiclass predictions:\n",
    "cf_mat = confusion_matrix(beans_gt, beans_preds)\n",
    "\n",
    "# plot the confusion matrix using seaborn heatmap\n",
    "sns.set(font_scale=1.4)  # adjust font size\n",
    "sns.heatmap(\n",
    "    cf_mat,\n",
    "    annot=True,\n",
    "    fmt=\"g\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=mc_labels,\n",
    "    yticklabels=mc_labels,\n",
    ")\n",
    "# add axis labels and title\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5231b568",
   "metadata": {
    "id": "5231b568"
   },
   "source": [
    "### Regression Metrics\n",
    "\n",
    "See <https://en.wikipedia.org/wiki/Regression_validation> for a good discussion of regression validation metrics.  We will look at a few of the most common here, focusing mostly on what is available in Scikit-Learn (See <https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics>).\n",
    "\n",
    "* **$R^2$ score, the coefficient of determination**\n",
    "  * $R^2(y, \\hat{y})=1-\\frac{\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2}{\\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)^2}$\n",
    "  * It represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.\n",
    "  * **It doesn't directly measure the performance of the model** as other metrics do.  You may want to use it in conjunction with another metric when measuring predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1352f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76c1352f",
    "outputId": "f1e6473d-adfb-4918-cf76-34a1bde7cf59"
   },
   "outputs": [],
   "source": [
    "print(f\"R^2: {r2_score(mpg_gt, mpg_preds) :0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f764a7d",
   "metadata": {
    "id": "1f764a7d"
   },
   "source": [
    "* **Mean Squared Error (MSE)**\n",
    "  * $\\operatorname{MSE}(y, \\hat{y})=\\frac{1}{n_{\\text {samples }}} \\sum_{i=0}^{n_{\\text {samples }}-1}\\left(y_i-\\hat{y}_i\\right)^2$\n",
    "  * MSE is probably the most common regression metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564215b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5564215b",
    "outputId": "c816ac89-5288-4ae5-bc14-cf3b52c33204"
   },
   "outputs": [],
   "source": [
    "print(f\"MSE: {mean_squared_error(mpg_gt, mpg_preds) :0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1f14a",
   "metadata": {
    "id": "8df1f14a"
   },
   "source": [
    "* **Mean Absolute Error (MAE)**\n",
    "  * $\\operatorname{MAE}(y, \\hat{y})=\\frac{1}{n_{\\text {samples }}} \\sum_{i=0}^{n_{\\text {samples }}-1}\\left|y_i-\\hat{y}_i\\right|$.\n",
    "  * MAE has the advantage that it is in the same _units_ as the dependent variable, so it is easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a723d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17a723d4",
    "outputId": "ebf37c1e-a07f-45b9-bb4f-7a439aae8128"
   },
   "outputs": [],
   "source": [
    "print(f\"MAE: {mean_absolute_error(mpg_gt, mpg_preds) :0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0733cc5c",
   "metadata": {
    "id": "0733cc5c"
   },
   "source": [
    "* **Mean Absolute Percentage Error (MAPE)**\n",
    "  * $\\operatorname{MAPE}(y, \\hat{y})=\\frac{1}{n_{\\text {samples }}} \\sum_{i=0}^{n_{\\text {samples }}-1} \\frac{\\left|y_i-\\hat{y}_i\\right|}{\\max \\left(\\epsilon,\\left|y_i\\right|\\right)}$\n",
    "  * A percentage is considered \"more interpretable\" in some circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd79e89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efd79e89",
    "outputId": "1d8f1a9f-dbc1-4b09-8346-e7092cd44c4a"
   },
   "outputs": [],
   "source": [
    "mape_raw = mean_absolute_percentage_error(mpg_gt, mpg_preds)\n",
    "print(f\"MAPE {mape_raw * 100 :0.1f}% ({mape_raw:0.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50be21",
   "metadata": {
    "id": "ba50be21"
   },
   "source": [
    "* **Visual evaluation of regression models**\n",
    "  * Plotting the residuals is a good visual tool for understanding how well your model is behaving.\n",
    "  * We can use the `PredictionErrorDisplay` object from Scikit-Learn to do this easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa7d33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "0faa7d33",
    "outputId": "1ba840c7-bde5-48b6-9089-308de97ea57d"
   },
   "outputs": [],
   "source": [
    "display = PredictionErrorDisplay(y_true=mpg_gt, y_pred=mpg_preds)\n",
    "display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac0fbad",
   "metadata": {
    "id": "2ac0fbad"
   },
   "source": [
    "Ideally, you want to see the residuals randomly distributed around the 0 line.  If you see a _pattern_ in the residuals, that is an indication that your model isn't doing a very good job.  In our case, **these residuals do not look good** -- we may have non-linear behavior, but we are using a simple linear model to make predictions.\n",
    "\n",
    "To solve this, we should probably consider transforming the predictors, the dependent variable, or both.  Or, we might want to choose a more complex model than a linear regression model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "formats": "md,ipynb"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
