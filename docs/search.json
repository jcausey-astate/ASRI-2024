[
  {
    "objectID": "index.html#june-06-classification-in-python-intermediate",
    "href": "index.html#june-06-classification-in-python-intermediate",
    "title": "Notebooks for ASRI-2024 Summer Workshops",
    "section": "June 06: Classification in Python (Intermediate)",
    "text": "June 06: Classification in Python (Intermediate)\n\n\nWorkshop version (with open exercises to complete)\nhttps://colab.research.google.com/github/jcausey-astate/ASRI-2024/blob/main/python_intermediate_classification_ASRI24.ipynb\n\n\nCompleted version for reference\nhttps://colab.research.google.com/github/jcausey-astate/ASRI-2024/blob/main/python_intermediate_classification_complete_ASRI24.ipynb"
  },
  {
    "objectID": "python_intermediate_classification_complete_ASRI24.html#asri-2024",
    "href": "python_intermediate_classification_complete_ASRI24.html#asri-2024",
    "title": "Classification in Python (Intermediate)",
    "section": "ASRI 2024",
    "text": "ASRI 2024\n\n\n\nClassification in Python (Intermediate)\n\n\n\n\n\nThe Palmer Archipelago penguins. Artwork by @allison_horst.\n\n\nThe notebook uses the following modules:\n\nmatplotlib : Provides basic graphing/charting.\nnumpy : Allows matrix and vector/array math.\npandas : Provides DataFrame functionality.\nseaborn : Works with matplotlib to provide nicer graphs.\nsklearn : Scikit-Learn provides machine learning and data manipulation tools.\n\nWe will rely heavily on the Scikit-Learn library for models, metrics, and experimental design tools. See the full documentation for this fantastic library at https://scikit-learn.org."
  },
  {
    "objectID": "python_intermediate_classification_complete_ASRI24.html#first-some-terms-and-definitions",
    "href": "python_intermediate_classification_complete_ASRI24.html#first-some-terms-and-definitions",
    "title": "Classification in Python (Intermediate)",
    "section": "First, some terms and definitions:",
    "text": "First, some terms and definitions:\nClassification is the process of determining a categorical label given the random variables for a given sample.\nCategorical values are allowed to take on only a finite (usually small) set of values. Categorical variables are usually non-numeric, but are sometimes encoded as numbers. Sometimes we refer to values of this type as labels, factors, or classes.\nA sample consists of all of the experimental information gathered for one item in the dataset. Sometimes a sample is called an object or item. Usually samples are arranged as rows in tabular datasets (CSV files, Excel spreadsheets, or similar).\nA random variable, sometimes called an input variable, measurement, or feature, is the recorded value for some property of the sample that was measured in the experiment, e.g.¬†‚Äúheight‚Äù, ‚Äúage‚Äù, ‚Äúflower color‚Äù, etc.\n\nYou have a classification problem if the dependent variable (output value) you are trying to predict is categorical.\nWe will focus first on classification problems where the random variables are continuous.\nContinuous values are numeric values that are allowed to take on any value within some range.\nAt the end, a section is provided with some tips for working with random variables that are categorical."
  },
  {
    "objectID": "python_intermediate_classification_complete_ASRI24.html#lets-see-some-code",
    "href": "python_intermediate_classification_complete_ASRI24.html#lets-see-some-code",
    "title": "Classification in Python (Intermediate)",
    "section": "Let‚Äôs see some code!",
    "text": "Let‚Äôs see some code!\nFirst, we have to import the modules, objects, and functions we will be using in this tutorial:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nThe Dataset\n‚ÑπÔ∏è The seaborn package has some sample datasets included.\nFor this tutorial, we will use the ‚ÄúPalmer Penguins‚Äù dataset, which is called penguins in the Seaborn index. We can load it with the load_dataset() function. It will load up as a Pandas DataFrame.\n\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n  \n    \n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nThe species column contains the value that we want to predict (it is our label column). Although we could use all the other columns as random variables (predictors), we will only focus on the numeric values for this part of the tutorial.\nIt will make things easier if we create variables to contain the name of the label column and the random variables. These can be used when we interact with Pandas DataFrames to quickly select those columns by name. This way, we don‚Äôt have to type the list of names often, and we don‚Äôt have to create a different data structure that only contains our variables of interest (although you could also do it that way).\n\nlabel_col = \"species\"\nrandom_var_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\",\n]\n\nLet‚Äôs use the info() DataFrame method to see what kinds of values we have, and whether there are any missing values.\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 18.9+ KB\n\n\nNotice that there are some missing values. We care most about the numeric columns for this example, so we want to drop any rows with missing values in those columns.\nThe dropna() method can do this. The subset parameter lets us specify which columns we care about (the random variables we specified earlier). We use axis=0 to indicate that we want to drop rows, not columns.\n\npenguins = penguins.dropna(subset=random_var_cols, axis=0)\n\n\n\nüìä Visualize Early, Visualize Often\nLet‚Äôs take a look at the dataset. We will plot two different ‚Äòviews‚Äô for comparison. the first will compare bill length with bill depth, and the second will compare bill length with flipper length.\nWe can color the datapoints according to species so that we can visually see how separable the different classes might be.\n\n# create a figure and two subplots\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\nplt.suptitle(\"Separating penguin species by bill measurements and/or flipper length.\")\n\n# create first scatterplot using Seaborn\nsns.scatterplot(\n    data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", ax=ax1\n)\nax1.set_title(\"Bill Length vs Bill Depth (mm)\")\n\n# create second scatterplot just like the first, but with different columns\nsns.scatterplot(\n    data=penguins, x=\"bill_length_mm\", y=\"flipper_length_mm\", hue=\"species\", ax=ax2\n)\nax2.set_title(\"Bill Length vs Flipper Length (mm)\")\n\n# adjust spacing between subplots\nplt.subplots_adjust(wspace=0.15)\n\n# show the plots\nplt.show()\n\n\n\n\n\n\n\n\nIf we look at these plots, it seems we can probably do a pretty good job of separating the three classes. We see that you could even get pretty good performance by drawing a few lines to separate the groups (in other words, a simple linear model might work reasonably well).\nTo see what a harder classification problem might look like, let‚Äôs draw another scatterplot where we compare the flipper length and the body mass:\n\nsns.scatterplot(data=penguins, x=\"flipper_length_mm\", y=\"body_mass_g\", hue=\"species\")\nplt.title(\"A more difficult problem...\")\nplt.show()\n\n\n\n\n\n\n\n\nIn this plot, it is very hard to see how we could separate the ‚ÄúAdelie‚Äù group from the ‚ÄúChinstrap‚Äù group. There is even some mixing between the ‚ÄúChinstrap‚Äù and ‚ÄúGentoo‚Äù groups.\n‚ú® Choosing the right random variables for prediction is vital. This is why it is a good idea to get to know your dataset early in the process! Visualize early, visualize often!"
  },
  {
    "objectID": "python_intermediate_classification_complete_ASRI24.html#lets-see-how-well-we-can-classify-with-a-linear-model.",
    "href": "python_intermediate_classification_complete_ASRI24.html#lets-see-how-well-we-can-classify-with-a-linear-model.",
    "title": "Classification in Python (Intermediate)",
    "section": "Let‚Äôs see how well we can classify with a linear model.",
    "text": "Let‚Äôs see how well we can classify with a linear model.\nFirst, we examine the LogisticRegression model (which is actually a classification model ‚Äì don‚Äôt let the name fool you).\nBased on the graphs we plotted above, let‚Äôs use the bill length and depth as our random variables. (‚ÑπÔ∏è : We could absolutely use all four random variables and it would probably do better, but using just two gives us a chance to discuss the performance with a very simple model.)\n\nrandom_var_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n]\n\nTo quickly determine if it will be suitable to this problem, we can use the cross_val_score() function from Scikit-Learn. This function wraps up a lot of functionality. It will set up a k-fold cross validation experiment (with default of \\(k=5\\), for five-fold CV). It will take the model of your choice and automatically train the model for each training fold, then predict the test cases and score the predictions on the test folds (with the accuracy metric by default).\nThe scores for each fold are returned. We can calculate and report the mean score over all five folds along with the standard deviation of the scores to see whether the model is able to do a good job in general, and how much variation we would expect for different training sets. Models should have high accuracy, and a low standard deviation would indicate that the model generalizes to new data very well. (A high standard deviation would indicate the model is unstable and doesn‚Äôt generalize well.)\n\nscores = cross_val_score(\n    LogisticRegression(max_iter=500), X=penguins[random_var_cols], y=penguins[label_col]\n)\nprint(\n    f\"mean: {scores.mean():0.3f}, std: {scores.std():0.3f}\"\n)  # print mean and standard deviation\n\nmean: 0.962, std: 0.020\n\n\nüéâ Wow! The linear model does a really good job on this problem!\nOK, that isn‚Äôt really surprising since we looked at the data first and we could see that some combinations of our random variables provided good linear separation between the groups. Still, it‚Äôs nice to see our intuition was correct.\nLet‚Äôs take a look at a different kind of model, just for comparison. A Random Forest model is a non-linear model that works well for lots of tasks. Scikit-Learn provides one called RandomForestClassifier.\nLet‚Äôs try it in exactly the same experimental setup we used for the linear model.\n\nscores = cross_val_score(\n    RandomForestClassifier(random_state=1),\n    X=penguins[random_var_cols],\n    y=penguins[label_col],\n)\nprint(f\"mean: {scores.mean():0.3f}, std: {scores.std():0.3f}\")\n\nmean: 0.968, std: 0.017\n\n\nHere, the random forest did about the same as the linear model (especially if we take the standard deviations of scores into account).\nIf we wanted to pick between these two models for this problem, we should probably choose the simpler one ‚Äì the logistic regression model.\nüí° The Principle of Parsimony says that given the choice between multiple models with similar performance, the best choice is usually the simplest model.\n‚ÑπÔ∏è One note:\nWe used random_state=1 to seed the random number generator within the model, causing it to produce identical results if we train it again on the same data. Random forests (as implied by their name) rely on some randomness during training, so you don‚Äôt expect to get the same performance every time. This makes reproducible results difficult.\nüí° By seeding the random state, we ‚Äúlock‚Äù it to a specific outcome (assuming no external changes). This way, others can reproduce our results in the future."
  },
  {
    "objectID": "python_intermediate_classification_complete_ASRI24.html#exploring-more-ways-to-characterize-classifier-performance.",
    "href": "python_intermediate_classification_complete_ASRI24.html#exploring-more-ways-to-characterize-classifier-performance.",
    "title": "Classification in Python (Intermediate)",
    "section": "Exploring more ways to characterize classifier performance.",
    "text": "Exploring more ways to characterize classifier performance.\n\nüìä Visualize!\nWhen the model is making incorrect predictions, sometimes we want to know which samples the model predicts incorrectly. This can help us diagnose whether the model is doing the best it can, whether the model is doing strange things, or even whether there might be a problem with the dataset itself.\nGenerally, a good starting point to diagnosing the mis-predicted values from a model is for us to visualize them in some way. Since this problem is easy to visualize as a 2-D scatterplot, we will use that as a way to see which samples the model is getting right vs.¬†wrong.\nTo start, let‚Äôs just split the dataset into a simple 80% / 20% train / test split. That means that we will reserve 20% of the samples for the test set, and the other 80% will be used for training. Scikit-Learn has a simple function for doing this (train_test_split()).\nThe function returns a training and testing dataframe (or matrix) given the original full dataset and the fraction you want to hold out for the test set.\n\ntrain_df, test_df = train_test_split(penguins, test_size=0.20, random_state=2)\n\nWe‚Äôll use the LogisticRegression model again, training it on the ‚Äútrain‚Äù partition (using the fit() method). Then, we‚Äôll predict the ‚Äútest‚Äù samples and calculate the (balanced) accuracy score.\n\nmodel = LogisticRegression(max_iter=500).fit(\n    X=train_df[random_var_cols], y=train_df[label_col]\n)\npreds = model.predict(test_df[random_var_cols])\nground_truth = test_df[label_col]\nprint(f\"{balanced_accuracy_score(ground_truth, preds):0.3f}\")\n\n0.932\n\n\nNow, let‚Äôs create the scatterplot that will show which samples were predicted incorrectly.\nWe can use color to indicate correct (green) and incorrect (red) predictions. We will also use different marker shapes to indicate the true class label so that we can see which ones are being predicted incorrectly and get a sense for why.\n\nplt.figure()\nfig = sns.scatterplot(\n    data=test_df,\n    x=\"bill_length_mm\",\n    y=\"bill_depth_mm\",\n    hue=(preds == ground_truth),\n    style=list(test_df[label_col].values),\n    markers=[\"o\", \"D\", \"s\"],\n    palette=[\"red\", \"green\"],\n)\nfig.legend()  # weird kludge:  The \"species\" title is shown by default, but just calling `legend()` removes it.  Why? ü§∑\nplt.title(\"Correct (green) and Incorrect (red) predictions.\")\nprint(test_df[label_col].unique())\n\n['Gentoo' 'Adelie' 'Chinstrap']\n\n\n\n\n\n\n\n\n\nThe green dots are samples that were correctly predicted and the red dots are incorrect predictions. There are only four incorrect predictions. Three of those are near the ‚Äúborder‚Äù between the two visual ‚Äúclusters‚Äù. That makes sense ‚Äì class mixing obviously occurs here. The other one is a Gentoo penguin that was incorrectly predicted (probably as a Chinstrap, since there are Chinstrap penguins nearby).\nAt this point, we will make the problem harder. Why? Well, it will be more interesting to explore correct / incorrect predictions if the model is not quite so good.\nThe flipper length and body mass do not combine to give very good separation, so we will choose those as our random variables from this point forward.\nü§î Of course, we would never choose worse predictors in any real analysis, but doing this can be useful as a learning exercise.\n\nrandom_var_cols = [\"flipper_length_mm\", \"body_mass_g\"]\n\nLet‚Äôs see how our new random variables perform with the same linear model as before:\n\nscores = cross_val_score(\n    LogisticRegression(max_iter=500), X=penguins[random_var_cols], y=penguins[label_col]\n)\nprint(f\"mean: {scores.mean():0.3f}, std: {scores.std():0.3f}\")\n\nmean: 0.690, std: 0.061\n\n\nMuch worse performance! üò¶ That‚Äôs bad‚Ä¶ But more interesting for exploring the performance metrics.\n\nscores = cross_val_score(\n    RandomForestClassifier(random_state=1),\n    X=penguins[random_var_cols],\n    y=penguins[label_col],\n)\nprint(f\"mean: {scores.mean():0.3f}, std: {scores.std():0.3f}\")\n\nmean: 0.772, std: 0.041\n\n\nThe random forest did quite a bit better here. We saw that these two variables don‚Äôt provide an obvious path for linear separation, but the random forest is not limited to linear decision boundaries.\nWe will use our simple 80%/20% train/test split from earlier and train the linear model using the new (worse) combination of random variables. First, train the model:\n\nmodel = LogisticRegression(max_iter=500).fit(\n    X=train_df[random_var_cols], y=train_df[label_col]\n)\npreds = model.predict(test_df[random_var_cols])\nground_truth = test_df[label_col]\nprint(f\"{balanced_accuracy_score(ground_truth, preds):0.3f}\")\n\n0.716\n\n\n\n\nConfusion Matrix\nNow, we can use another visualization technique to discuss the performance characteristics. This technique is called a confusion matrix. It shows the number of samples from each true label that were predicted as each possible output label. Seaborn makes very nice confusion matrix plots.\n\ncm = confusion_matrix(ground_truth, preds)\n\n# plot the confusion matrix using seaborn heatmap\nsns.set(font_scale=1.4)  # adjust font size\nlabels = model.classes_\nsns.heatmap(\n    cm, annot=True, fmt=\"g\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels\n)\n\n# add axis labels and title\nplt.xlabel(\"Predicted label\")\nplt.ylabel(\"True label\")\nplt.title(\"Confusion Matrix\")\n\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\nCorrect predictions appear along the diagonal (upper-left to lower-right). All of the other squares represent incorrect predictions.\nHere, the linear model did very well with predicting Gentoo penguins when it saw a real Gentoo. But it also incorrectly guessed that 3 Chinstraps and 1 Adelie were Gentoos as well (False Positives).\nAs for the Chinstraps, we correctly classified 6 of them, but we incorrectly labeled 7 Chinstraps as Adelie and 3 as Gentoo (False Negatives).\n\n\nNon-visual metrics\nLet‚Äôs look at other classification metrics.\nScikit-Learn provides several metrics appropriate for evaluating classification models. You can see the list at https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics.\nWe will start with the classification_report() function, which combines several popular metrics into a single report.\n\n# We get precision, recall, and f1-score from the classification report.\n# You can also get these individually from functions in sklearn.metrics.\nprint(classification_report(ground_truth, preds))\n\n              precision    recall  f1-score   support\n\n      Adelie       0.77      0.77      0.77        31\n   Chinstrap       0.50      0.38      0.43        16\n      Gentoo       0.85      1.00      0.92        22\n\n    accuracy                           0.75        69\n   macro avg       0.71      0.72      0.71        69\nweighted avg       0.73      0.75      0.74        69\n\n\n\nWhat about binary classification?\nSo far, we‚Äôve been performing multi-class classification: There were three possible classes {Adelie, Chinstrap, Gentoo}, and each sample could only be a member of a single class.\nMany classification problems can be expressed as binary classification problems. That just means that there are two classes (and all samples must be one or the other, but not both).\nSome metrics make sense with binary problems, but not with multi-class problems. Let‚Äôs change our dataset to make it into a binary classification problem. To do this, we will simply change classification to answer the question ‚ÄúChinstrap or not?‚Äù. So, our new labels will be {Chinstrap, Other}. To do this, we will make a copy of our dataset and modify the species column to reflect the binary labeling.\n\n# Make a binary dataset by splitting the \"Adelie\" and \"Gentoo\" penguins\n# away from the \"Chinstrap\" penguins.\nbinary_penguins = penguins.copy()\nbinary_penguins.loc[binary_penguins[\"species\"] != \"Chinstrap\", \"species\"] = \"Other\"\n\n# Create new train/test split with the new dataset.  (80%/20% as before)\nb_train_df, b_test_df = train_test_split(\n    binary_penguins, test_size=0.20, random_state=2, stratify=binary_penguins[\"species\"]\n)\n# Fit a linear model to the new dataset\nmodel = LogisticRegression(max_iter=500).fit(\n    X=b_train_df[random_var_cols], y=b_train_df[label_col]\n)\n# And predict on the test set.\npreds = model.predict(b_test_df[random_var_cols])\nground_truth = b_test_df[label_col]\nprint(f\"Acc: {accuracy_score(ground_truth, preds):0.3f}\")\nprint(classification_report(ground_truth, preds))\n\nAcc: 0.783\n              precision    recall  f1-score   support\n\n   Chinstrap       0.33      0.07      0.12        14\n       Other       0.80      0.96      0.88        55\n\n    accuracy                           0.78        69\n   macro avg       0.57      0.52      0.50        69\nweighted avg       0.71      0.78      0.72        69\n\n\n\nWe can see that the binary accuracy is about 78%.\n\nReceiver Operating Characteristic (ROC) Curve and Area Under the ROC Curve (AUC)\nA common way of comparing binary classifier is by visually interpreting a performance curve called the Receiver Operating Characteristic (ROC) curve, or by numerically interpreting the area under the ROC curve (AUC or AUROC).\nTo create an ROC curve, we need to predict the probability that each sample belongs to the ‚Äúpositive‚Äù class. In Scikit-Learn compatible models, you can use the predict_proba() method to do this.\n\nprobas = model.predict_proba(b_test_df[random_var_cols])\n\npredict_proba gives a score for each class. For binary problems, we only need the score for the first class (the ‚Äúpositive‚Äù class). We will select that by slicing off the first column from all the rows in probas:\n\nprobas_pos = probas[:, 0]\n\nWe can make a binary (1,0) ground truth by comparing the labels with the first class in our model (which we will consider the ‚Äúpositive‚Äù class):\n\nground_truth = b_test_df[label_col] == model.classes_[0]\n\nThe ROC curve plots the True-Positive Rate (tpr) against the False-Positive Rate (fpr) given all possible thresholds (from 0.0 to 1.0). The roc_auc_score() function from Scikit-Learn can compute the tpr and fpr scores for us, given the ground truth and predicted probabilities:\n\nfpr, tpr, _ = roc_curve(ground_truth, probas_pos)\n\nAnd the roc_auc_score() will calculate the area under the ROC curve, given the same information:\n\nauc = roc_auc_score(ground_truth, probas_pos)\n\nNow, let‚Äôs plot the ROC curve and display the AUC in the legend using Matplotlib:\n\nplt.plot(fpr, tpr, label=f\"auc={auc:0.3f}\")\nplt.xlabel(\"fpr\")\nplt.ylabel(\"tpr\")\nplt.legend(loc=4)\nplt.show()\n\n\n\n\n\n\n\n\nSince we might want to compare multiple models on the same figure, let‚Äôs make a function that will take a dictionary of the form {name: model} of models, the training and testing samples and labels, and plot the ROC curve for all models:\n\n# plots multiple models on the same ROC plot and compare them visually:\ndef multi_auc_comparison(models, X_train, y_train, X_test, y_test):\n    for name in models:\n        model = models[name]\n        model.fit(X_train, y_train)\n        probas = model.predict_proba(X_test)[:, 0]\n        fpr, tpr, _ = roc_curve(y_test, probas)\n        auc = roc_auc_score(y_test, probas)\n        plt.plot(fpr, tpr, label=f\"{name} AUC: {auc:0.3f}\")\n        plt.xlabel(\"fpr\")\n        plt.ylabel(\"tpr\")\n        plt.title(\"ROC Curve Comparision\")\n    plt.legend(loc=4)\n    plt.show()\n\nLet‚Äôs see it in action by comparing the linear logistic regression model to the random forest model:\n\nmulti_auc_comparison(\n    {\n        \"LR\": LogisticRegression(max_iter=500),\n        \"RF\": RandomForestClassifier(random_state=1),\n    },\n    b_train_df[random_var_cols],\n    b_train_df[label_col],\n    b_test_df[random_var_cols],\n    ground_truth,\n)\n\n\n\n\n\n\n\n\nGenerally, the higher AUC is better, but we can see from the ROC curve plot that there is some tradeoff in the performance characteristics (tradeoff between false positives and false negatives)."
  },
  {
    "objectID": "python_intermediate_classification_complete_ASRI24.html#working-with-categorical-features",
    "href": "python_intermediate_classification_complete_ASRI24.html#working-with-categorical-features",
    "title": "Classification in Python (Intermediate)",
    "section": "Working with categorical features",
    "text": "Working with categorical features\nSo far, we only used numeric features for our predictors. But, the Palmer Penguins dataset also contains some categorical features:\n\nisland - three levels: [‚ÄòBiscoe‚Äô, ‚ÄòDream‚Äô, ‚ÄòTorgersen‚Äô]\nsex - two levels [‚ÄòMale‚Äô, ‚ÄòFemale‚Äô]\n\nBinary variables (with only two levels) can be re-encoded as 0 and 1 and used essentially the same as a continuous numeric variable.\nVariables with more than two levels require a little more thought. You could encode them using different numeric levels (e.g.¬†{-1, 0, 1}), but this might not always work well. A common approach to multi-level categorical variables is to one-hot encode them.\nOne-hot encoding is an encoding technique in which a variable with \\(N\\) levels is split into \\(N\\) new pseudo-variables where each is a binary variable encoded as 1 or 0.\nLet‚Äôs see how our island variable might look if it were one-hot encoded:\nBefore\nspecies island    bill_length_mm bill_depth_mm ...\nAdelie  Biscoe    38.8           17.2          ...\nAdelie  Torgersen 40.3           18.0          ...\nAdelie  Torgersen 39.1           18.7          ...\nAdelie  Biscoe    37.8           18.3          ...\nAdelie  Dream     39.5           17.8          ...\nAdelie  Biscoe    38.2           18.1          ...\nAdelie  Torgersen 36.7           19.3          ...\nAdelie  Dream     37.2           18.1          ...\nAfter\nspecies island_Biscoe island_Dream island_Torgersen bill_length_mm bill_depth_mm ...\nAdelie  1             0            0                38.8           17.2          ...\nAdelie  0             0            1                40.3           18.0          ...\nAdelie  0             0            1                39.1           18.7          ...\nAdelie  1             0            0                37.8           18.3          ...\nAdelie  0             1            0                39.5           17.8          ...\nAdelie  1             0            0                38.2           18.1          ...\nAdelie  0             0            1                36.7           19.3          ...\nAdelie  0             1            0                37.2           18.1          ...\nHeres the code:\nPandas can do this in a dataframe by using the get_dummies() method. You provide a prefix (like \"island\") and the existing levels are used to complete the new column names.\nBy default, the values will be Boolean (True, False), but we can use dtype=int to make them integers. (‚ÑπÔ∏è You don‚Äôt have to do this - the Boolean values will convert automatically when needed. We do it here just to be explicit about how the categories are becoming numbers.)\n\npenguins_encoded = pd.get_dummies(\n    penguins, columns=[\"island\"], prefix=\"island\", dtype=int\n)\npenguins_encoded.head()\n\n\n  \n    \n\n\n\n\n\n\nspecies\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nisland_Biscoe\nisland_Dream\nisland_Torgersen\n\n\n\n\n0\nAdelie\n39.1\n18.7\n181.0\n3750.0\nMale\n0\n0\n1\n\n\n1\nAdelie\n39.5\n17.4\n186.0\n3800.0\nFemale\n0\n0\n1\n\n\n2\nAdelie\n40.3\n18.0\n195.0\n3250.0\nFemale\n0\n0\n1\n\n\n4\nAdelie\n36.7\n19.3\n193.0\n3450.0\nFemale\n0\n0\n1\n\n\n5\nAdelie\n39.3\n20.6\n190.0\n3650.0\nMale\n0\n0\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nAs you can see, the island column is gone and replaced with three binary columns. Let‚Äôs query some random rows to see more than just Torgersen island:\n\npenguins_encoded.sample(n=5, random_state=0)\n\n\n  \n    \n\n\n\n\n\n\nspecies\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nisland_Biscoe\nisland_Dream\nisland_Torgersen\n\n\n\n\n93\nAdelie\n39.6\n18.1\n186.0\n4450.0\nMale\n0\n1\n0\n\n\n281\nGentoo\n46.2\n14.9\n221.0\n5300.0\nMale\n1\n0\n0\n\n\n133\nAdelie\n37.5\n18.5\n199.0\n4475.0\nMale\n0\n1\n0\n\n\n280\nGentoo\n45.3\n13.8\n208.0\n4200.0\nFemale\n1\n0\n0\n\n\n7\nAdelie\n39.2\n19.6\n195.0\n4675.0\nMale\n0\n0\n1\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nThere‚Äôs More Than One Way to Do It\nYou can also use the OneHotEncoder from Scikit-Learn to encode a single variable. It is not as simple as the Pandas method shown above when you have categorical and numeric values in a dataframe, but it works great when you need to one-hot encode your output label. (‚ÑπÔ∏è Some models require that categorical outputs are one-hot encoded. Scikit-Learn models usually don‚Äôt require this.)\nHere‚Äôs how it would look to encode the island column and print five rows.\n\nencoded_island = (\n    OneHotEncoder().fit_transform(penguins[[\"island\"]]).toarray()\n)  # NOTE: the extra [] is necessary to get the correct shape for the single columns we are selecting.\n\n# The following lines are all related to printing five example rows.  The line above did all the hard work.\nnp.random.seed(0)\nidx = np.random.permutation(np.arange(len(encoded_island)))\nprint(encoded_island[idx[:5]])\n\n[[0. 1. 0.]\n [1. 0. 0.]\n [0. 1. 0.]\n [1. 0. 0.]\n [0. 0. 1.]]\n\n\nThere are several also other approaches to encoding categorical values.\nYou can learn a lot more here: https://www.kaggle.com/code/arashnic/an-overview-of-categorical-encoding-methods"
  },
  {
    "objectID": "python_beginner_choosing_a_model_complete_ASRI24.html#asri-2024",
    "href": "python_beginner_choosing_a_model_complete_ASRI24.html#asri-2024",
    "title": "Choosing and Evaluating Machine Learning Models in Python (Beginner)",
    "section": "ASRI 2024",
    "text": "ASRI 2024\n\nThis notebook shows some introductory examples from the ‚ÄúChoosing and Evaluating Machine Learning Models in Python‚Äù workshop session.\nThe notebook uses the following modules:\n\nmatplotlib : Provides basic graphing/charting.\nnumpy : Allows matrix and vector/array math.\npandas : Provides DataFrame functionality.\nseaborn : Works with matplotlib to provide nicer graphs.\nsklearn : Scikit-Learn provides machine learning and data manipulation tools.\n\nWe will rely heavily on the Scikit-Learn library for models, metrics, and experimental design tools. See the full documentation for this fantastic library at https://scikit-learn.org.\n\nIn this workshop, we will focus on the big picture questions of model selection and evaluation:\n\nModel Selection : How to you begin to choose a model?\n\nPrediction vs Exploration\nRegression vs Classification\nSupervised vs Unsupervised\nGeneral model types and assumptions\nSome common and useful models to consider.\n\nEvaluation Metrics : How do you know when your model is doing a good job?"
  },
  {
    "objectID": "python_beginner_choosing_a_model_complete_ASRI24.html#model-selection-how-to-you-begin-to-choose-a-model",
    "href": "python_beginner_choosing_a_model_complete_ASRI24.html#model-selection-how-to-you-begin-to-choose-a-model",
    "title": "Choosing and Evaluating Machine Learning Models in Python (Beginner)",
    "section": "Model Selection : How to you begin to choose a model?",
    "text": "Model Selection : How to you begin to choose a model?\n\nPrediction vs Exploration\nPrediction means that you have some value, label, or other concept that you want to be able to predict in the future, given some measurements or information (we will call these ‚Äúfeatures‚Äù or ‚Äúrandom variables‚Äù or ‚Äúpredictors‚Äù) from a sample. For example, you might want to predict whether a mushroom is edible based on observed facts about it such as gill type, spore print color, cap color, etc.\nA predictive analysis implies that you already know what you are looking for ‚Äì you have a value that is important to you (but might be hard to directly measure), and some predictive measurements that you believe might be able to determine that value.\nExploration means that you are more interested in using a dataset to discover interesting facts that might suggest something about how the world works. For example, you might want to figure out whether you can identify separate groups of plants within a field that are growing differently in some way from other plants.\nAn exploratory analysis is an exercise in curiosity. You are looking for interesting relationships or facts in a dataset that might be useful to you in some way.\n\n\nRegression vs Classification\nRegression models try to compute a continuous output value (dependent variable) for a given sample.\n\nContinuous values are numeric values that are allowed to take on any value within some range.\nA sample consists of all of the experimental information gathered for one item in the dataset. Sometimes a sample is called an object or item. Usually samples are arranged as rows in tabular datasets (CSV files, Excel spreadsheets, or similar).\n\nClassification is the process of determining a categorical label given the random variables for a given sample.\n\nCategorical values are allowed to take on only a finite (usually small) set of values. Categorical variables are usually non-numeric, but are sometimes encoded as numbers. Sometimes we refer to values of this type as labels, factors, or classes.\n\nA random variable, sometimes called an input variable, measurement, or feature, is the recorded value for some property of the sample that was measured in the experiment, e.g.¬†‚Äúheight‚Äù, ‚Äúage‚Äù, ‚Äúflower color‚Äù, etc.\n\n\nSupervised vs Unsupervised\nSupervised models require a training dataset that has the correct output value (dependent variable) already determined. The model is trained (or ‚Äúfit‚Äù) so that the correct output value is produced given the random variables in the sample for each sample in the training data.\n\nIn Scikit-Learn: https://scikit-learn.org/stable/supervised_learning.html\n\nUnsupervised models are able to produce an estimate for the dependent variable without the need to provide a pre-labeled training dataset. Some unsupervised models focus on establishing similarities between groups of samples, rather than computing an output value. (Clustering models do this, for example.)\n\nIn Scikit-Learn: https://scikit-learn.org/stable/unsupervised_learning.html\n\n\n\nGeneral model types and assumptions\n\nParametric models\n\nParametric models are statistical models that make assumptions about the underlying distribution of the data being modeled. In parametric models, the relationship between the input variables and the output variable is represented by a fixed equation with a predefined number of parameters.\n\nNonparametric models\n\nNonparametric models are a type of statistical model that does not make any assumptions about the underlying distribution of the data being modeled. These models do not have a fixed number of parameters, and the complexity of the model increases as the amount of data increases. Nonparametric models are particularly useful when the underlying distribution of the data is unknown or when the data is very complex and nonlinear.\n\n\n\n\nLinear models\n\nAs the name implies, linear models imply a linear relationship between input features and the dependent variable: \\(y=mx+b\\).\nLinear models can be extended to allow the dependent variable to follow a non-normal distribution through the use of a link function. Models of this type are called generalized linear models (GLMs); the Logistic Regression model is a common example.\n\nKernel-based models\n\nKernel-based models transform the input variables into a potentially non-linear high-dimensional space before applying multi-linear modeling methods to solve for the dependent variable. This allows the model to work on problems where simple linear models would not perform well.\nA common example is the Support Vector Machine (SVM).\n\nTree-based models\n\nTree-based models are based around the concept of a decision tree, which is a structure in which random variables are tested (a decision is made) and the next action to take depends on the result of that decision. Drawing such a model on paper results in a tree-like structure where each interior node is a decision and each leaf node is a label or regression result.\n\nNeural Network models\n\nNeural-network models rely on layers of artificial ‚Äúneurons‚Äù, each of which is a linear model followed by a nonlinear activation function. Neural networks tend to contain large numbers of trainable parameters compared to other parametric models, and are capable of fitting very complex datasets with very complex outputs, but require intensive training to reach good performance.\nNeural networks are often characterized by the number of layers (more than one hidden layer ‚û°Ô∏è ‚Äúdeep learning‚Äù) and the way layers are connected (‚Äúfully connected‚Äù, ‚Äúconvolutional‚Äù, ‚Äúrecurrent‚Äù, ‚Äútransformer‚Äù, etc.).\n\nClustering models\n\nClustering models are used primarily for exploratory analysis. They do not require a known labeling for the data, and work by establishing groupings within the samples based on some similarity measure.\nCommon examples are k-Means, Hierarchical Clustering, DBSCAN, Mean Shift.\n\nEnsemble models\n\nEnsemble models models that use collections of simpler models internally, then reach a final output label or regression result by consensus. By utilizing many simple models, ensembles are often able to both reach higher peak performance and also lower variance than single models.\nCommon examples are Random Forests, Boosting models (AdaBoost), and Gradient Boosted Tree models (XGBoost, LightGBM)."
  },
  {
    "objectID": "python_beginner_choosing_a_model_complete_ASRI24.html#evaluation-metrics-how-do-you-know-when-your-model-is-doing-a-good-job",
    "href": "python_beginner_choosing_a_model_complete_ASRI24.html#evaluation-metrics-how-do-you-know-when-your-model-is-doing-a-good-job",
    "title": "Choosing and Evaluating Machine Learning Models in Python (Beginner)",
    "section": "Evaluation Metrics : How do you know when your model is doing a good job?",
    "text": "Evaluation Metrics : How do you know when your model is doing a good job?\n\nLet‚Äôs see some code! First, we will import the python packages we need for this workshop:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom scipy.io import arff\nfrom sklearn import datasets\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import PredictionErrorDisplay\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nNow, we need to set up some datasets. You can run these cells and essentially ignore the code there for now ‚Äì come back to it later after you‚Äôve looked at the discussion of the different metrics below.\n\n# The classification dataset is \"Dry Bean\" from https://archive.ics.uci.edu/dataset/602/dry+bean+dataset\n# Download the dataset (but only if it hasn't already been done).\n! [[ -f dry+bean+dataset.zip ]] || wget https://archive.ics.uci.edu/static/public/602/dry+bean+dataset.zip\n! [[ -d DryBeanDataset ]] || unzip dry+bean+dataset.zip\n\n# Load the data.\ndata = arff.loadarff(\"DryBeanDataset/Dry_Bean_Dataset.arff\")\nbeans = pd.DataFrame(data[0])\nbeans[\"Class\"] = beans[\"Class\"].str.decode(\"utf-8\")\n\n# We will keep only three classes: \"SIRA, CALI, and BOMBAY\".\nbeans = beans[beans[\"Class\"].isin([\"SIRA\", \"CALI\", \"BOMBAY\"])]\n# Put the label column name and predictor column names in variables:\nbeans_label_col = \"Class\"\n# Choose only two variables for this tutorial (and not the best ones, by far).\nbeans_var_cols = [\"Extent\", \"Eccentricity\"]\n# We will also create a binary version (one with only two classes).\nbin_beans = beans.copy()\nbin_beans.loc[bin_beans[\"Class\"] != \"SIRA\", \"Class\"] = \"NOT_SIRA\"\n\n# Make a simple 80/20 train/test split:\nbeans_train, beans_test = train_test_split(\n    beans, test_size=0.20, stratify=beans[\"Class\"], random_state=1\n)\nbin_beans_train, bin_beans_test = train_test_split(\n    bin_beans, test_size=0.20, stratify=beans[\"Class\"], random_state=1\n)\n\nbeans_gt = beans_test[beans_label_col].values\nbin_beans_gt = bin_beans_test[beans_label_col].values\n\n# Print counts of each bean type:\nprint(\n    f\"Counts by class in train set (multiclass):\\n{beans_train['Class'].value_counts()}\"\n)\nprint(\n    f\"\\n\\nCounts by class in test set (multiclass):\\n{beans_test['Class'].value_counts()}\"\n)\nprint(\n    f\"\\n\\nCounts by class in train set (binary):\\n{bin_beans_train['Class'].value_counts()}\"\n)\nprint(\n    f\"\\n\\nCounts by class in test set (binary):\\n{bin_beans_test['Class'].value_counts()}\"\n)\n# Create a simple linear (logistic) regression classifier...\nlm = LogisticRegression(max_iter=1000)\n# ... And train it on the multiclass problem...\nlm.fit(beans_train[beans_var_cols], beans_train[beans_label_col])\nbeans_preds = lm.predict(beans_test[beans_var_cols])\nbeans_probas = lm.predict_proba(beans_test[beans_var_cols])\nmc_labels = lm.classes_\n# ... And also on the binary classification problem.\nlm.fit(bin_beans_train[beans_var_cols], bin_beans_train[beans_label_col])\nbin_beans_preds = lm.predict(bin_beans_test[beans_var_cols])\nbin_beans_probas = lm.predict_proba(bin_beans_test[beans_var_cols])\n\nCounts by class in train set (multiclass):\nClass\nSIRA      2109\nCALI      1304\nBOMBAY     417\nName: count, dtype: int64\n\n\nCounts by class in test set (multiclass):\nClass\nSIRA      527\nCALI      326\nBOMBAY    105\nName: count, dtype: int64\n\n\nCounts by class in train set (binary):\nClass\nSIRA        2109\nNOT_SIRA    1721\nName: count, dtype: int64\n\n\nCounts by class in test set (binary):\nClass\nSIRA        527\nNOT_SIRA    431\nName: count, dtype: int64\n\n\n\n# The regression dataset is \"MPG\" from https://archive.ics.uci.edu/dataset/9/auto+mpg\n# Download the dataset (but only if it hasn't already been done).\n! [[ -f auto+mpg.zip ]] || wget https://archive.ics.uci.edu/static/public/9/auto+mpg.zip\n! [[ -f auto-mpg.data ]] || unzip auto+mpg.zip\n\n# Load the data.\nmpg = pd.read_csv(\n    \"auto-mpg.data\",\n    header=None,\n    delim_whitespace=True,\n    usecols=[0, 2, 3, 4],\n    names=[\"mpg\", \"disp\", \"hp\", \"weight\"],\n    na_values=[\"?\"],\n)\nmpg = mpg.dropna(axis=0)\nmpg_label_col = \"mpg\"\nmpg_var_cols = [\"disp\", \"hp\", \"weight\"]\n# We should really standardize the predictors, but for now, we'll ignore that.\n\n# Make a simple 80/20 train/test split:\nmpg_train, mpg_test = train_test_split(mpg, test_size=0.20, random_state=1)\nmpg_gt = mpg_test[mpg_label_col].values\n\nrm = LinearRegression()\nrm.fit(mpg_train[mpg_var_cols], mpg_train[mpg_label_col])\nmpg_preds = rm.predict(mpg_test[mpg_var_cols])\n\n/var/folders/lc/rg61fs4569dcvt21xd0nffx40000gn/T/ipykernel_97808/881814137.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n  mpg = pd.read_csv(\n\n\n\n\nClassification Metrics\nA very good source of information about metrics for binary classifiers is available on Wikipedia:\nhttps://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers\n\nSome definitions\n\nCondition Positive (P)\n\nThe number of real positive cases in the data\n\nCondition Negative (N)\n\nThe number of real negative cases in the data\n\nTrue Positive (TP)\n\nA test result that correctly indicates the presence of a condition or characteristic\n\nTrue Negative (TN)\n\nA test result that correctly indicates the absence of a condition or characteristic\n\nFalse Positive (FP)\n\nA test result which wrongly indicates that a particular condition or attribute is present\n\nFalse Negative (FN)\n\nA test result which wrongly indicates that a particular condition or attribute is absent\n\n\n\n\nMetrics with examples\n\nRecall (REC)\n\n\\(\\frac{\\mathrm{TP}}{\\mathrm{P}}\\) or \\(\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}\\)\nIn Scikit-Learn: sklearn.metrics.recall_score()\n\n\n\nprint(\n    f\"REC (binary): {recall_score(bin_beans_gt, bin_beans_preds, pos_label='SIRA'):0.3f}\"\n)\nprint(\n    f\"REC (multiclass): {recall_score(beans_gt, beans_preds, average='weighted'):0.3f}\"\n)\n\nREC (binary): 0.831\nREC (multiclass): 0.735\n\n\n\nPrecision (PREC)\n\n\\(\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}\\)\n\n\n\nprint(\n    f\"PREC (binary): {precision_score(bin_beans_gt, bin_beans_preds, pos_label='SIRA'):0.3f}\"\n)\nprint(\n    f\"PREC (multiclass): {precision_score(beans_gt, beans_preds, average='weighted'):0.3f}\"\n)\n\nPREC (binary): 0.741\nPREC (multiclass): 0.657\n\n\n/Users/jcausey/A-State Dropbox/Jason Causey/ASRI/ASRI_2024/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n\n\n\nAccuracy (ACC)\n\n\\(\\frac{\\mathrm{TP}+\\mathrm{TN}}{\\mathrm{P}+\\mathrm{N}}\\) or \\(\\frac{\\mathrm{TP}+\\mathrm{TN}}{\\mathrm{TP}+\\mathrm{TN}+\\mathrm{FP}+\\mathrm{FN}}\\)\n\n\n\nprint(f\"ACC (binary): {accuracy_score(bin_beans_gt, bin_beans_preds):0.3f}\")\nprint(f\"ACC (multiclass): {accuracy_score(beans_gt, beans_preds):0.3f}\")\n\nACC (binary): 0.747\nACC (multiclass): 0.735\n\n\n\nBalanced Accuracy (BA)\n\n\\(\\frac{1}{2}(\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}} + \\frac{\\mathrm{TN}}{\\mathrm{TN}+\\mathrm{FP}})\\)\nFor multiclass, see https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score\n\n\n\nprint(f\"BA (binary): {balanced_accuracy_score(bin_beans_gt, bin_beans_preds):0.3f}\")\nprint(f\"BA (multiclass): {balanced_accuracy_score(beans_gt, beans_preds):0.3f}\")\n\nBA (binary): 0.738\nBA (multiclass): 0.535\n\n\n\nF1 score (F1) is the harmonic mean of precision and recall.\n\n\\(\\frac{2 \\mathrm{TP}}{2 \\mathrm{TP}+\\mathrm{FP}+\\mathrm{FN}}\\)\n\n\n\nprint(f\"F1 (binary): {f1_score(bin_beans_gt, bin_beans_preds, pos_label='SIRA'):0.3f}\")\nprint(f\"F1 (multiclass): {f1_score(beans_gt, beans_preds, average='weighted'):0.3f}\")\n\nF1 (binary): 0.784\nF1 (multiclass): 0.690\n\n\n\nROC curve and Area Under the ROC Curve (AUROC)\n\nhttps://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/\nIntegrating the area under the ROC curve gives you the numeric metric AUROC. It is in the range \\([0,1]\\), and higher is better.\n\n\n\n# Area under the ROC Curve:\nauroc_bin = roc_auc_score(\n    bin_beans_gt, bin_beans_probas[:, 1]\n)  # 'SIRA' is in column index 1\nauroc_mc = roc_auc_score(beans_gt, beans_probas, multi_class=\"ovr\")\n\nprint(f\"AUROC (binary): {auroc_bin:0.3f}\")\nprint(f\"AUROC (multiclass): {auroc_mc:0.3f}\")\n\n\n# A function to plot the ROC curve:\ndef plot_roc_curve(y_true, y_pred_proba, auc, desc=\"\"):\n    fpr, tpr, _ = roc_curve(y_true, y_pred_proba, pos_label=\"SIRA\")\n    plt.plot(fpr, tpr, label=f\"AUC={auc:0.3f}\")\n    plt.title(f\"ROC {desc}\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.legend(loc=4)\n    plt.show()\n\n\nplot_roc_curve(bin_beans_gt, bin_beans_probas[:, 1], auroc_bin, \"(binary)\")\n\nAUROC (binary): 0.806\nAUROC (multiclass): 0.786\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\n\nVisual grid showing the correct labeling (rows) versus the predicted labeling (columns), with the number in each cell representing the number of samples predicted for each (ground_truth / prediction) combination.\nThe major diagonal shows correct labelings, all others show incorrect labelings.\nMost often used in multi-class evaluations.\n\n\n\n# Confusion matrix for the multiclass predictions:\ncf_mat = confusion_matrix(beans_gt, beans_preds)\n\n# plot the confusion matrix using seaborn heatmap\nsns.set(font_scale=1.4)  # adjust font size\nsns.heatmap(\n    cf_mat,\n    annot=True,\n    fmt=\"g\",\n    cmap=\"Blues\",\n    xticklabels=mc_labels,\n    yticklabels=mc_labels,\n)\n# add axis labels and title\nplt.xlabel(\"Predicted label\")\nplt.ylabel(\"True label\")\nplt.title(\"Confusion Matrix\")\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nRegression Metrics\nSee https://en.wikipedia.org/wiki/Regression_validation for a good discussion of regression validation metrics. We will look at a few of the most common here, focusing mostly on what is available in Scikit-Learn (See https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics).\n\n\\(R^2\\) score, the coefficient of determination\n\n\\(R^2(y, \\hat{y})=1-\\frac{\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2}{\\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)^2}\\)\nIt represents the proportion of variance (of y) that has been explained by the independent variables in the model. It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.\nIt doesn‚Äôt directly measure the performance of the model as other metrics do. You may want to use it in conjunction with another metric when measuring predictive performance.\n\n\n\nprint(f\"R^2: {r2_score(mpg_gt, mpg_preds) :0.3f}\")\n\nR^2: 0.718\n\n\n\nMean Squared Error (MSE)\n\n\\(\\operatorname{MSE}(y, \\hat{y})=\\frac{1}{n_{\\text {samples }}} \\sum_{i=0}^{n_{\\text {samples }}-1}\\left(y_i-\\hat{y}_i\\right)^2\\)\nMSE is probably the most common regression metric.\n\n\n\nprint(f\"MSE: {mean_squared_error(mpg_gt, mpg_preds) :0.3f}\")\n\nMSE: 19.535\n\n\n\nMean Absolute Error (MAE)\n\n\\(\\operatorname{MAE}(y, \\hat{y})=\\frac{1}{n_{\\text {samples }}} \\sum_{i=0}^{n_{\\text {samples }}-1}\\left|y_i-\\hat{y}_i\\right|\\).\nMAE has the advantage that it is in the same units as the dependent variable, so it is easy to interpret.\n\n\n\nprint(f\"MAE: {mean_absolute_error(mpg_gt, mpg_preds) :0.3f}\")\n\nMAE: 3.254\n\n\n\nMean Absolute Percentage Error (MAPE)\n\n\\(\\operatorname{MAPE}(y, \\hat{y})=\\frac{1}{n_{\\text {samples }}} \\sum_{i=0}^{n_{\\text {samples }}-1} \\frac{\\left|y_i-\\hat{y}_i\\right|}{\\max \\left(\\epsilon,\\left|y_i\\right|\\right)}\\)\nA percentage is considered ‚Äúmore interpretable‚Äù in some circumstances.\n\n\n\nmape_raw = mean_absolute_percentage_error(mpg_gt, mpg_preds)\nprint(f\"MAPE {mape_raw * 100 :0.1f}% ({mape_raw:0.3f})\")\n\nMAPE 13.6% (0.136)\n\n\n\nVisual evaluation of regression models\n\nPlotting the residuals is a good visual tool for understanding how well your model is behaving.\nWe can use the PredictionErrorDisplay object from Scikit-Learn to do this easily:\n\n\n\ndisplay = PredictionErrorDisplay(y_true=mpg_gt, y_pred=mpg_preds)\ndisplay.plot()\n\n\n\n\n\n\n\n\nIdeally, you want to see the residuals randomly distributed around the 0 line. If you see a pattern in the residuals, that is an indication that your model isn‚Äôt doing a very good job. In our case, these residuals do not look good ‚Äì we may have non-linear behavior, but we are using a simple linear model to make predictions.\nTo solve this, we should probably consider transforming the predictors, the dependent variable, or both. Or, we might want to choose a more complex model than a linear regression model."
  },
  {
    "objectID": "python_intermediate_clustering_complete_ASRI24.html#asri-2024",
    "href": "python_intermediate_clustering_complete_ASRI24.html#asri-2024",
    "title": "Clustering in Python (Intermediate)",
    "section": "ASRI 2024",
    "text": "ASRI 2024\n\n\n\nClassification in Python (Intermediate)\n\n\nThis notebook shows some introductory examples from the ‚ÄúClustering in Python‚Äù workshop session.\nThe notebook uses the following modules:\n\nmatplotlib : Provides basic graphing/charting.\nnumpy : Allows matrix and vector/array math.\npandas : Provides DataFrame functionality.\nscipy : SciPy provides algorithms for optimization, algebraic equations, statistics and many other classes of problems. We will use it for building dendrograms.\nseaborn : Works with matplotlib to provide nicer graphs.\nsklearn : Scikit-Learn provides machine learning and data manipulation tools.\n\nWe will rely heavily on the Scikit-Learn library for models, metrics, and experimental design tools. See the full documentation for this fantastic library at https://scikit-learn.org.\n\nClustering is an unsupervised learning technique for exploring relationships between the random variables in a dataset. We use a clustering analysis to try to identify groups or similar objects in datasets with two or more random variables.\nIn this tutorial, we will look at some commonly-used clustering techniques:\n\nk-Means Clustering\n\nProbably the most common clustering technique.\nGood for finding clusters that look like ‚Äúblobs‚Äù when visualized.\nYou need to know the number of clusters in advance.\n\nMean Shift Clustering\n\nDoesn‚Äôt require assumptions about the number of clusters.\nSomewhat robust when clusters are not simple ‚Äúblobs‚Äù.\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n\nClusters areas of similar density, regardless of cluster ‚Äúshape‚Äù.\nWorks for situations where clusters don‚Äôt look like separate ‚Äúblobs‚Äù.\nYou don‚Äôt need to know the number of clusters in advance.\n\nAgglomerative Hierarchical Clustering\n\nEstablishes relationships at all levels of the comparison metric between all samples.\nCan be visualized as a dendrogram.\nAllows you to determine the ‚Äúright‚Äù number of clusters by examining the dendrogram.\n\nGaussian Mixture Model\n\nAssumes the data can be represented as some number of multi-dimensional Gaussian distributions.\nWorks well when the data are ‚Äúblob‚Äù shaped, even when they overlap ‚Äì especially if the density of the blobs differ.\nIdeally, you need to know how many ‚Äúblobs‚Äù to expect.\n\n\n‚ÑπÔ∏è More information about these techniques and many others is available at https://scikit-learn.org/stable/modules/clustering.html."
  },
  {
    "objectID": "python_intermediate_clustering_complete_ASRI24.html#lets-see-some-code",
    "href": "python_intermediate_clustering_complete_ASRI24.html#lets-see-some-code",
    "title": "Clustering in Python (Intermediate)",
    "section": "Let‚Äôs see some code",
    "text": "Let‚Äôs see some code\nFirst, we have to import the modules, objects, and functions we will be using in this tutorial:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn import cluster\nfrom sklearn import datasets\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import MeanShift\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler"
  },
  {
    "objectID": "python_intermediate_clustering_complete_ASRI24.html#comparing-clustering-techniques",
    "href": "python_intermediate_clustering_complete_ASRI24.html#comparing-clustering-techniques",
    "title": "Clustering in Python (Intermediate)",
    "section": "Comparing clustering techniques",
    "text": "Comparing clustering techniques\nüí° There is no single perfect clustering algorithm. Different algorithms make different assumptions with regards to what you already know about your dataset.\nFor that reason, we will use a few different datasets to illustrate the strengths and weaknesses of each clustering algorithm.\nHere are the datasets we will use:\n\nPalmer Penguins : This real-world dataset contains measurements of the bill length versus the flipper length of three species of penguins at the Palmer Antarctic research station. https://allisonhorst.github.io/palmerpenguins/articles/intro.html\nMoons : This simulated dataset consists of two crescent ‚Äúmoon‚Äù shapes, with one inverted so that there is no way to linearly separate the two groups, but they are visually separate.\nCircles : This simulated dataset consists of two concentric rings, representing noisy values that are in one of two groups, assigned by the radius of the ring in which the sample is located.\nDensity Blobs : This simulated dataset will consist of three ‚Äúblobs‚Äù with widely varying density and size characteristics. The blobs overlap so that they are not perfectly visually separable.\n\nThe following code block creates the simulated datasets and loads the Palmer Penguins dataset and selects the two predictors we will use for this tutorial.\n\nn_samples = 500\n\n# penguins : this is a real dataset, so we will load it and select a useful set of\n#            features.  We will also store the labels.\npenguins = sns.load_dataset(\"penguins\").dropna(\n    axis=0, subset=[\"bill_length_mm\", \"flipper_length_mm\"]\n)\npenguins_encoder = LabelEncoder().fit(penguins[\"species\"].values)\npenguins_classes = penguins_encoder.classes_\npenguins_labels = penguins_encoder.transform(penguins[\"species\"].values)\npenguins = penguins[[\"bill_length_mm\", \"flipper_length_mm\"]].values\npenguins = StandardScaler().fit_transform(penguins)\n\n# moons : two crescent \"moon\" shapes, with one inverted so that there is no way to\n# linearly separate the two groups, but they are visually separate.\nmoons, moons_labels = datasets.make_moons(n_samples=n_samples, noise=0.065)\nmoons = StandardScaler().fit_transform(moons)\n\n# circles : two concentric rings, representing noisy values that are in one of two\n# groups, assigned by the radius of the ring in which the sample is located.\ncircles, circles_labels = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.065\n)\ncircles = StandardScaler().fit_transform(circles)\n\n# density blobs : three \"blobs\" with widely varying density and size characteristics.\n# The blobs overlap so that they are not perfectly visually separable.\ndensity_blobs, density_blobs_labels = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.9, 0.5, 2.7], random_state=4\n)\ndensity_blobs = StandardScaler().fit_transform(density_blobs)\n\nüí° To make understanding the data easier, we need a way to visualize the datasets, plus give hints about what the ‚Äúexpected‚Äù labeling would be as well as what a clustering technique assigns as labels.\nWe will create a function that can take a list of datasets with corresponding names, labels, and styles. The function will use the Seaborn scatterplot() function to plot the datasets in a grid.\n\ndef plot_all(data_list, names=None, label_list=None, style_list=None):\n    \"\"\"\n    Plot scatterplots for all the 2-D datasets in `data_list`.\n    `names` provide a title for each plot\n    `label_list` is a list of label assignments for the samples in the data list.\n    `style_list` is a list of marker shape assignments for the samples in the data list.\n    \"\"\"\n    nrows = int(round(len(data_list) / 2, 0))\n    fig, axs = plt.subplots(nrows=nrows, ncols=2, figsize=(12, 10))\n    for r in range(nrows):\n        for c in range(2):\n            hue = label_list[r * 2 + c] if label_list is not None else None\n            style = style_list[r * 2 + c] if style_list is not None else None\n            title = names[r * 2 + c] if names is not None else None\n            sns.scatterplot(\n                x=data_list[r * 2 + c][:, 0],\n                y=data_list[r * 2 + c][:, 1],\n                hue=hue,\n                style=style,\n                palette=sns.color_palette(n_colors=len(set(hue))),\n                legend=False,\n                ax=axs[r, c],\n            ).set(title=title)\n    plt.subplots_adjust(wspace=0.50)\n    plt.show()\n\nNow, let‚Äôs use the plot_all() function to plot our four datasets. We will use the ‚Äúreal‚Äù labels to color the points so that you can see what the ‚Äúideal‚Äù labeling would be.\n\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\"Penguins\", \"Moons\", \"Circles\", \"Density Blobs\"],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nüí° Keep in mind that in many real-world clustering applications, we will not know what the ‚Äúcorrect‚Äù or ‚Äúideal‚Äù labels are.\nüí° That is the ‚Äúmain idea‚Äù of clustering: It is a technique for suggesting a possible labeling for a dataset by examining relationships between the samples.\n\nk-Means\nLet‚Äôs start with k-Means clustering. k-Means is probably the most well-known clustering technique because it is simple, intuitive to understand, and it works well for clustering lots of datasets where we can visually identify ‚Äúgroups‚Äù in a scatterplot.\nk-Means works by first choosing a value \\(k\\) that represents the desired number of clusters. You need to already know something about the dataset to correctly choose \\(k\\) (for example, you may visualize it first and choose the number that seem right from looking at the scatterplot). There are also some techniques for choosing \\(k\\) if you have no idea of what to try. (See: https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb)\nLet‚Äôs see how k-Means would label each of our test datasets. We ‚Äúknow‚Äù from looking at the scatterplots that there are three groups in the ‚Äúpenguins‚Äù and ‚Äúdensity blobs‚Äù datasets, and there are two groups in the ‚Äúmoons‚Äù and ‚Äúcircles‚Äù datasets. So, we will choose \\(k\\) for each dataset to match our expectation.\n\nkmeans_2 = KMeans(n_clusters=2, random_state=1)  # k-Means with 2 clusters\nkmeans_3 = KMeans(n_clusters=3, random_state=1)  # k-Means with 3 clusters\n\n# We apply k-Means to each dataset with the `fit_transform()` method.  We \"believe\"\n# that \"moons\" and \"circles\" should have k=2 and \"penguins\" and \"density_blobs\"\n# should have k=3.\n#\npenguins_kmeans = kmeans_3.fit_predict(penguins)\nmoons_kmeans = kmeans_2.fit_predict(moons)\ncircles_kmeans = kmeans_2.fit_predict(circles)\ndensity_blobs_kmeans = kmeans_3.fit_predict(density_blobs)\n\n# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n# to determine the shapes of the markers in the plots here so that we can compare:\n#\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"k-Means Penguins (k=3)\",\n        \"k-Means Moons (k=2)\",\n        \"k-Means Circles (k=2)\",\n        \"k-Means Density Blobs (k=3)\",\n    ],\n    [penguins_kmeans, moons_kmeans, circles_kmeans, density_blobs_kmeans],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î We can see that k-Means does a reasonable job of finding the three groups in the ‚Äúpenguins‚Äù dataset. One of those groups (the Chinstrap penguins) is difficult to separate even with more complicated methods. The method does a good job of separating the two ‚Äúeasier‚Äù groups (Adelie and Gentoo).\n‚ùì What if we chose \\(k\\) incorrectly? Let‚Äôs see by first assuming \\(k=2\\) for all four datasets:\n\n# What if we believe there are 2 clusters in each of the datasets?\n#\nkmeans_2 = KMeans(n_clusters=2, random_state=1)  # k-Means with 2 clusters\n\n# We apply k-Means to each dataset with the `fit_transform()` method.\n#\npenguins_kmeans = kmeans_2.fit_predict(penguins)\nmoons_kmeans = kmeans_2.fit_predict(moons)\ncircles_kmeans = kmeans_2.fit_predict(circles)\ndensity_blobs_kmeans = kmeans_2.fit_predict(density_blobs)\n\n# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n# to determine the shapes of the markers in the plots here so that we can compare:\n#\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"k-Means Penguins (k=2)\",\n        \"k-Means Moons (k=2)\",\n        \"k-Means Circles (k=2)\",\n        \"k-Means Density Blobs (k=2)\",\n    ],\n    [penguins_kmeans, moons_kmeans, circles_kmeans, density_blobs_kmeans],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\n‚ùì What do you think about these clusters?\nNow, let‚Äôs incorrectly assume \\(k=4\\) for all four datasets:\n\n# What if we believe there are 4 clusters in all of the datasets?\n#\nkmeans_4 = KMeans(n_clusters=4, random_state=1)  # k-Means with 4 clusters\n\n# We apply k-Means to each dataset with the `fit_transform()` method.\n#\npenguins_kmeans = kmeans_4.fit_predict(penguins)\nmoons_kmeans = kmeans_4.fit_predict(moons)\ncircles_kmeans = kmeans_4.fit_predict(circles)\ndensity_blobs_kmeans = kmeans_4.fit_predict(density_blobs)\n\n# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n# to determine the shapes of the markers in the plots here so that we can compare:\n#\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"k-Means Penguins (k=4)\",\n        \"k-Means Moons (k=4)\",\n        \"k-Means Circles (k=4)\",\n        \"k-Means Density Blobs (k=4)\",\n    ],\n    [penguins_kmeans, moons_kmeans, circles_kmeans, density_blobs_kmeans],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\n‚ÑπÔ∏è With k-Means, the clustering is stochastic, meaning that it will be different each time you run it (unless you set the random_state to make it reproducible).\nLet‚Äôs see that in action:\n\n# What if we believe there are 4 clusters in all of the datasets?\n#\nkmeans_4 = KMeans(n_clusters=4)  # k-Means with 4 clusters, no fixed random state.\n\n# We apply k-Means to each dataset with the `fit_transform()` method.\n#\npenguins_kmeans_1 = kmeans_4.fit_predict(penguins)\npenguins_kmeans_2 = kmeans_4.fit_predict(penguins)\npenguins_kmeans_3 = kmeans_4.fit_predict(penguins)\npenguins_kmeans_4 = kmeans_4.fit_predict(penguins)\n\n# Now let's see how the proposed clusters look.  We will use the \"ideal\" labels\n# to determine the shapes of the markers in the plots here so that we can compare:\n#\nplot_all(\n    [penguins, penguins, penguins, penguins],\n    [\n        \"k-Means Penguins (k=4)\",\n        \"k-Means Penguins (k=4)\",\n        \"k-Means Penguins (k=4)\",\n        \"k-Means Penguins (k=4)\",\n    ],\n    [penguins_kmeans_1, penguins_kmeans_2, penguins_kmeans_3, penguins_kmeans_4],\n    [penguins_labels, penguins_labels, penguins_labels, penguins_labels],\n)\n\n\n\n\n\n\n\n\nüí° Keep in mind when using k-Means: A single run does not give you any indication confidence that the labeling was stable. (A stable labeling is an indication that the labeling is more likely ‚Äúcorrect‚Äù with respect to some real-world relationship.)\nYou can run the clustering several times and compare the stability of the cluster assignments to get an idea of whether the algorithms is seeing ‚Äúreal‚Äù groups or just randomly assigning them based on the initial seed points chosen.\nFor more information on measuring the stability of a clustering assignment, see the following article:\nhttps://amueller.github.io/aml/04-model-evaluation/17-cluster-evaluation.html\n\n\nMean-Shift Clustering\nMean-Shift clustering provides a powerful technique for clustering a dataset when we don‚Äôt already know the number of clusters.\n\n# For each dataset, we need to estimate the bandwidth parameter, then create the\n# MeanShift object.  To automate this, we will wrap the two steps up into a function.\ndef build_MeanShift(X, quantile=0.25):\n    bandwidth = cluster.estimate_bandwidth(X, quantile=quantile)\n    return MeanShift(bandwidth=bandwidth)\n\n\n# Now build a MeanShift clusterer for each dataset and use it to fit/predict.\npenguins_ms = build_MeanShift(penguins).fit_predict(penguins)\nmoons_ms = build_MeanShift(moons).fit_predict(moons)\ncircles_ms = build_MeanShift(circles, quantile=0.2).fit_predict(circles)\ndensity_blobs_ms = build_MeanShift(density_blobs, quantile=0.1).fit_predict(\n    density_blobs\n)\n\n# Now plot them:\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"Mean Shift Penguins\",\n        \"Mean Shift Moons\",\n        \"Mean Shift Circles\",\n        \"Mean Shift Density Blobs\",\n    ],\n    [penguins_ms, moons_ms, circles_ms, density_blobs_ms],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î Mean-Shift did a very good job on the Penguins dataset, without needing to know that there were three groups! It didn‚Äôt do so well with the other datasets though. Circles and Moons are not ‚Äúblob-like‚Äù enough for this technique to work well. In the density blobs dataset, it did (sort of) find the small dense blob‚Ä¶ But at the expense of finding many more small clusters that don‚Äôt really exist.\n‚ÑπÔ∏è The bandwidth parameter is the most important tuning parameter for this algorithm, and it can be tricky to get just right. The estimate_bandwidth() function is provided to help find a good bandwidth value, but it also has a tuning parameter quantile that can be a bit finicky. Some trial-and-error may be required to get results that seem correct.\n\n\nDBSCAN\nThe DBSCAN technique looks for regions of similar density, and assumes that those regions represent groups of samples. It is also capable of automatically identifying outliers (or ‚Äúnoise points‚Äù) that may not belong to any group.\n\n# Like MeanShift, there is an important tuning parameter ('eps') for DBSCAN.\n# Use DBSCAN to cluster each dataset:\npenguins_DBSCAN = DBSCAN(eps=0.25).fit_predict(penguins)\nmoons_DBSCAN = DBSCAN(eps=0.3).fit_predict(moons)\ncircles_DBSCAN = DBSCAN(eps=0.28).fit_predict(circles)\ndensity_blobs_DBSCAN = DBSCAN(eps=0.1).fit_predict(density_blobs)\n\n# Now plot them:\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"DBSCAN Penguins\",\n        \"DBSCAN Moons\",\n        \"DBSCAN Circles\",\n        \"DBSCAN Density Blobs\",\n    ],\n    [penguins_DBSCAN, moons_DBSCAN, circles_DBSCAN, density_blobs_DBSCAN],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î We can see that DBSCAN did a very good job on the ‚Äústrange‚Äù datasets (moons and circles). It also did a decent job with the Penguins dataset. The performance on the density blobs data is interesting: It identified the small dense cluster, but found spurious extra clusters as well. It also failed to see the two larger clusters as separate objects.\n‚ÑπÔ∏è DBSCAN requires careful tuning of the eps parameter, which is related to the expected density of clusters. When there is a wide variation in the density of ‚Äúreal‚Äù clusters, DBSCAN can fail to perform well (as it did here). However, if there is a sparse background of noisy points with some clusters of similar density, DBSCAN will find those clusters even if they are not ‚Äúblob-shaped‚Äù.\n\n\nAgglomerative Hierarchical Clustering\n\n# Use agglomerative clustering to cluster each dataset:\npenguins_ag = AgglomerativeClustering(n_clusters=3, linkage=\"ward\").fit_predict(\n    penguins\n)\nmoons_ag = AgglomerativeClustering(n_clusters=2, linkage=\"single\").fit_predict(moons)\ncircles_ag = AgglomerativeClustering(n_clusters=2, linkage=\"single\").fit_predict(\n    circles\n)\ndensity_blobs_ag = AgglomerativeClustering(n_clusters=3, linkage=\"ward\").fit_predict(\n    density_blobs\n)\n\n# Now plot them:\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"Hierarchical Penguins\",\n        \"Hierarchical Moons\",\n        \"Hierarchical Circles\",\n        \"Hierarchical Density Blobs\",\n    ],\n    [penguins_ag, moons_ag, circles_ag, density_blobs_ag],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î The hierarchical clustering technique did a pretty good job on all the datasets, but we had to do some tuning. Specifically, the choice of linkage can make a big difference, as can the choice of metric (which we did not tune here). We also provided the model with our desired number of clusters. Agglomerative Clustering will work without the number of clusters provided, but it might not produce the desired result. If you know the expected number of clusters, it is very helpful to provide it.\n‚ÑπÔ∏è Another thing you can do with hierarchical clustering is to visualize the relationships between the samples as a dendrogram. A dendrogram is a tree-like representation that shows every possible clustering from a single group to \\(N\\) groups (where \\(N\\) is the total number of samples). The height of the vertical lines represent changes in similarity between splits.\nYou can use this visualization to get an idea of how many clusters should exist in the dataset. This can be used to determine \\(k\\) before performing k-Means clustering, or for other models that require knowing the number of groups beforehand.\nüíÅ The dendrogram visualization is available from SciPy. (See: https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram)\nTo see it in action, we will generate dendrogram for the ‚Äúdensity blobs‚Äù dataset:\n\nlinkage_matrix = linkage(density_blobs, \"ward\")\nplt.figure(figsize=(10, 9))\nplt.title(\"Hierarchical Density Blobs Dendrogram\")\nplt.ylabel(\"distance (Ward)\")\ndendrogram(linkage_matrix, no_labels=True)\nplt.show()\n\n\n\n\n\n\n\n\nBy examining the vertical lines, you can determine the ‚Äúbest‚Äù number of clusters by choosing where to ‚Äúcut‚Äù the graph with an imaginary horizontal line.\n\n\nGaussian Mixture Model\nThe Gaussian Mixture Model (GMM) works by assuming that clusters are multi-dimensional Gaussian distributions (which look like roughly ‚Äúcircular‚Äù blobs that are denser in the middle). It is quite flexible if you have ‚Äúblob-like‚Äù clusters, and GMM-like clusters occur frequently in natural datasets.\n\nfrom sklearn.mixture import GaussianMixture\n\n# Use a GMM to cluster each dataset.  We assume that penguins and\n# density blobs have three clusters, and that moons and circles have two.\n#\npenguins_ag = GaussianMixture(n_components=3).fit_predict(penguins)\nmoons_ag = GaussianMixture(n_components=2).fit_predict(moons)\ncircles_ag = GaussianMixture(n_components=2).fit_predict(circles)\ndensity_blobs_ag = GaussianMixture(n_components=3).fit_predict(density_blobs)\n\n# Now plot them:\nplot_all(\n    [penguins, moons, circles, density_blobs],\n    [\n        \"GMM Penguins\",\n        \"GMM Moons\",\n        \"GMM Circles\",\n        \"GMM Density Blobs\",\n    ],\n    [penguins_ag, moons_ag, circles_ag, density_blobs_ag],\n    [penguins_labels, moons_labels, circles_labels, density_blobs_labels],\n)\n\n\n\n\n\n\n\n\nü§î The GMM did a very good job on the ‚Äúblob-like‚Äù datasets ‚Äúpenguins‚Äù and ‚Äúdensity blobs‚Äù. In fact, it probably did best on density blobs (versus the other methods we tried).\nHowever, it does not do as well with the datasets whose values aren‚Äôt ‚Äúblob-like‚Äù in shape."
  }
]